---
title: 'graph-pes: graph-based machine-learning models for potential-energy surfaces'
tags:
  - Python
  - machine learning
  - graphs
  - interatomic potentials
  - force fields
  - molecular dynamics
  - chemistry
  - materials science
  - foundation models
authors:
  - name: John L. A. Gardner
    orcid: 0009-0006-7377-7146
    affiliation: 1 
  - name: Volker L. Deringer
    orcid: 0000-0001-6873-0278
    affiliation: 1
affiliations:
 - name: Inorganic Chemistry Laboratory, Department of Chemistry, University of Oxford, Oxford OX1 3QR, United Kingdom
   index: 1
date: 10/01/2025
bibliography: paper.bib
---

# Summary

We present `graph-pes`, an open-source toolkit for accelerating the development, training, and deployment of machine-learned interatomic potential (MLIP) models that act on graph representations of atomic structures. The `graph-pes` toolkit comprises three components:
 
1. **The `graph_pes` Python package**: a modular framework containing all functionality required to build, train, and evaluate graph-based MLIPs. The package includes a mature data pipeline for converting atomic structures into graph representations (`AtomicGraph`s), a fully featured base class for MLIP implementations (`GraphPESModel`), and a suite of common data manipulation routines and model building blocks. We provide independent re-implementations of common MLIP architectures out-of-the-box, as well as interfaces to several foundational MLIP models (see below).
 
2. **The `graph-pes-train` command-line interface** (CLI): a convenience tool for training graph-based MLIPs on datasets of labelled atomic structures directly from the command line. The tool is compatible with any `GraphPESModel` (i.e., those defined in `graph-pes`, user-designed ones, and foundation models) and is designed to be easily extensible via custom loss functions, optimisers, datasets, and more.

3. **Molecular-dynamics drivers** for popular MD engines that allow any `GraphPESModel` to be used in GPU-accelerated MD simulations. We currently provide a `pair style` for use in LAMMPS [@Thompson-22-02], a `GraphPESCalculator` for use in ASE [@Larsen-17-06], and an integration with the `torch-sim` package [@torch-sim].

# Statement of need

In recent years, machine-learned PES models, commonly referred to as machine-learned interatomic potentials (MLIPs), have become central tools for computational chemistry and materials science [@Deringer-19-11].

These models are trained on labels generated by quantum-mechanical methods, but scale much more favourably with system size, making it possible to simulate the dynamics of large systems (millions of atoms and more) over extended timescales. In this way, MLIPs are facilitating the study of complex physical and chemical phenomena at the atomic scale, in turn driving the generation of novel insight and understanding. 

Many flavours of MLIPs exist, and with them have arisen a variety of software packages that are typically tailored to training specific architectures (see examples below). Given their unique specialisations, these individual software implementations do not normally conform to a common interface, making it difficult for practitioners to migrate their training and validation pipelines between different model architectures.

![Schematic overview of the functionality of `graph-pes`. The core components are highlighted in colour. Red: The `AtomicGraph` class is used to represent atomic structures and incorporates the notion of locality via a neighbour list. Blue: The GraphPESModel class is the general base class for all models defined using `graph-pes`. We provide independent, stand-alone (re-) implementations of popular architectures, together with interfaces allowing access to several pre-trained and "foundational" MLIP models. Custom, user-defined MLIPs are easy to create, and are fully compatible with the rest of `graph-pes`'s functionality. Green: `graph-pes` includes a CLI for easy training, and interfaces to multiple external simulation tools for evaluating MLIP models](./overview.png)

`graph-pes` provides a **unified interface and software framework** for defining, training, and  working with graph-based MLIP models. This reduces the barrier to entry for researchers wanting to implement new MLIP architectures, and allows practitioners to easily explore different MLIP architectures for their specific use cases: training scripts require as little as one line of code to swap between model architectures, while validation scripts can be written in an architecture-agnostic manner, with `LAMMPS` input scripts, `ASE` calculators, and `torch-sim` simulations requiring no changes other than pointing a different model file.

# Features and implementation

Below, we briefly summarise the key design choices, components, and features of `graph-pes`, and further emphasise the advantages of having a unified interface for all MLIP architectures.
For an extended overview of the `graph-pes` framework, and comprehensive documentation, please visit [this URL](https://jla-gardner.github.io/graph-pes/).

## Representing atomic structures with graphs

An atomic structure containing $N$ atoms is completely defined by the positions of its atoms ($\mathbf{R} \in \mathbb{R}^{N \times 3}$), and their chemical identities ($Z \in \mathbb{Z_+}^N$).[^1] 
A graph representation of the atomic structure incorporates this complete description, together with an edge list ($\mathbf{E} \in \mathbb{Z}^{E \times 2}$) indicating which atoms are within the local environment of others (defined, for instance, using a fixed cut-off radius). 
The resulting graph, $G = \{\mathbf{R}, Z, \mathbf{E}\}$, is thus an extremely general representation of chemical structure with a built-in definition of locality.

We therefore define the `AtomicGraph` class as the base data structure in `graph-pes`, and provide convenience methods to convert these to and from `ase.Atoms` objects [@HjorthLarsen-17-06].

Writing performant code to implement common graph-based operations can be challenging: we therefore provide optimised implementations to access many derived properties (such as `number_of_atoms`, `neighbour_distances`, and `number_of_neighbours`) as well as to perform common graph-based operations (such as `index_over_neighbours`, `sum_over_neighbours`, and `sum_per_structure`). 
All of these functions work for both single and batched graph instances, simplifying the implementation of new MLIP models, and making their forwards passes easily readable.

[^1]: This statement is assuming that the structure is isolated: defining a periodic structure requires the trivial addition of a unit cell and periodic boundary conditions.

## Model implementations

All MLIP models in `graph-pes` are implemented as subclasses of the `GraphPESModel` base class, which itself inherits from the `torch.nn.Module` class.
These models take an (optionally batched) `AtomicGraph` as input, and are able to return a collection of PES property predictions, including the total energy, atomic forces, and cell stress tensors.

Implementations need only define a forward pass that returns a local energy for each atom in the graph, or a total energy for the entire structure; we use the functionality from `torch.autograd` to automatically calculate force and stress tensors in a conservative manner [@Paszke-19].
For faster modelling, we also fully support models that return direct force and stress tensor predictions (e.g., `TensorNet` or `orb-v3-*` with their optional direct force readout heads).

Building on the `GraphPESModel` class, we provide independent (re-) implementations of popular MLIP architectures, including `PaiNN` [@Schutt-21-06], `EDDP` [@Pickard-22-07], `NequIP` [@Batzner-22-05], `MACE` [@Batatia-23-01], and `TensorNet` [@Simeon-23-06]. We use building blocks provided by the `e3nn` [@Geiger-22-07] package to implement models that act on spherical tensor decompositions.

In addition, we provide an `AdditionModel` implementation, which makes energy, force and stress predictions as a sum over several independent models. 
This allows `graph-pes` to add the following features onto any other model architecture:

- **Offset energies**. A common feature of quantum-mechanical labelling methods is that the "reference energy" of an isolated atom is (i) non-zero, (ii) different for each element, and (iii) varies between different levels of theory/method. We implement the `EnergyOffset` model to account for this, removing the need to include these contributions in any other model implementation.

- **Pair repulsions.** Adding smooth, short-ranged, exponentially repulsive pair repulsion contributions on top of many-bodied model predictions guarantee correct model behaviour in the short-range limit, and act to stabilise MD simulations. We implement the `LennardJones`, `Morse`, and `ZBLCoreRepulsion`, all with smoothed cutoffs and optionally learnable parameters, to trivially add these repulsions to any other model implementation.

## Training and validation

We provide the `graph-pes-train` CLI tool for training any `GraphPESModel` on datasets of labelled atomic structures.
Configuration for this tool is specified via a hierarchically-structured YAML file, with separate sections for specifying the `model`, `data`, `loss` and `fitting` parameters.
As well as training from scratch, we also support the loading of pre-trained models, allowing for fine-tuning of existing models on new datasets. In this manner, a wide variety of pre-train/fine-tune strategies are supported, including synthetic pre-training [@Gardner-24-01], foundation model fine-tuning (see below), and frozen transfer learning [@Radova-25-02].

Under the hood, `graph-pes-train` builds upon the `PyTorch Lightning` [@Lightning] training loop, allowing for a wide range of advanced features, including learning rate scheduling, stochastic weight averaging, gradient clipping, and more. 
By using the `data2objects` package [@data2objects] to parse configuration files, we also support the use of arbitrary, user-defined components, including custom loss functions, model architectures, optimisers, and datasets.

Because all models conform to the same interface, all training features can be used with any model architecture. Similarly, all downstream model uses can be written in an architecture-agnostic manner, allowing for MD, relaxations, and other scripts to be written once, and then used with any MLIP architecture for extended validation beyond simple error metrics [@Morrow-23-03].

## Easy access to foundation models

A topical and recent area of research is the development of universal or "foundational" MLIPs that can describe the potential-energy surface of a wide range of systems. `graph-pes` integrates directly with the `mace-torch`, `mattersim` and `orb-models` packages to provide access to, among others, the `MACE-MP` [@Batatia-24-03], `MatterSim` [@Yang-24-05], `orb-v2` [@Neumann-24-10], `MACE-OFF` [@Kovacs-25-01],  `Egret-v1` [@Mann-25-05], and `orb-v3` [@Rhodes-25-04] families of models. Each of these integrations generates `GraphPESModels` that are directly compatible with all `graph-pes` features, including fine-tuning, validation pipelines, and MD simulations.

# Related work

`graph-pes` is beginning to drive a substantial number of projects within our research group, and we hope that it will be useful to many others. In recent preprints, we have described the use of `graph-pes` for fitting NequIP models to datasets created using the `autoplex` software [@Liu-24-12], for assessing the zero-shot performance of different graph-network MLIP models [@Mahmoud-25-02], and for fine-tuning and distilling atomistic foundation models [@Gardner-25-06].

Relevant alternative packages that offer training and validation functionaltiy for _specific_ ML-PES architectures include: `schnetpack` [@schutt2019schnetpack; @schutt2023schnetpack], `deepmd-kit` [@Wang-18-07; @Zeng-23-08], `nequip` [@Batzner-22-05], `mace-torch` [@Batatia-23-01], `torchmd-net` [@TorchMDNet], and `fairchem` [@fairchem]. The `MatterTune` package [@Kong-25-04] provides a unified interface for fine-tuning atomistic foundation models, but does not create models with a common interface, or allow for training arbitrary MLIP architectures from scratch.

# Acknowledgements

We thank Zoé Faure Beaulieu, Krystian Gierczak, and Daniel Thomas du Toit for early testing and feedback.
J.L.A.G. acknowledges a UKRI Linacre - The EPA Cephalosporin Scholarship, support from an EPSRC DTP award [Grant Number EP/T517811/1], and from the Department of Chemistry, University of Oxford.
This work was supported by UK Research and Innovation [grant number EP/X016188/1].

# References
