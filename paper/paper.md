---
title: 'graph-pes: graph-based machine-learning models for potential-energy surfaces'
tags:
  - Python
  - machine learning
  - graphs
  - interatomic potentials
  - force fields
  - molecular dynamics
  - chemistry
  - materials science
  - foundation models
authors:
  - name: John L. A. Gardner
    orcid: 0009-0006-7377-7146
    affiliation: 1 
  - name: Volker L. Deringer
    orcid: 0000-0001-6873-0278
    affiliation: 1
affiliations:
 - name: Department of Chemistry, University of Oxford, Oxford, United Kingdom
   index: 1
date: 01/08/2025
bibliography: paper.bib
---

# Summary

We present `graph-pes`, an open-source toolkit for accelerating the development, training, and deployment of machine-learned interatomic potential (MLIP) models that act on graph representations of atomic structures. The toolkit comprises three components:
 
1. **The `graph_pes` Python package**: a modular framework containing all functionality required to build, train, and evaluate graph-based MLIPs. The package includes a mature data pipeline for converting atomic structures into graph representations (`AtomicGraph`s), a fully featured base class for MLIP implementations (`GraphPESModel`), and a suite of common data manipulation routines and model building blocks.
 
2. **The `graph-pes-train` command-line interface** (CLI): a convenience tool for training graph-based MLIPs on datasets of labelled atomic structures directly from the command line. The tool is compatible with any `GraphPESModel` (i.e., those defined in `graph-pes`, user-designed ones, and foundation models) and is designed to be easily extensible via custom loss functions, optimisers, datasets, and more.

3. **Molecular-dynamics drivers** that allow any `GraphPESModel` to be used in GPU-accelerated MD simulations. We currently provide a `pair style` for LAMMPS [@Thompson-22-02], a `GraphPESCalculator` for ASE [@Larsen-17-06], and an integration with the `torch-sim` package [@torch-sim].

# Statement of need

In recent years, machine-learned PES models have become central tools for computational chemistry and materials science [@Deringer-19-11].
These models are trained on labels generated by quantum-mechanical methods, but scale much more favourably with system size, making it possible to simulate the dynamics of large systems over extended timescales.

Many flavours of MLIPs exist, and with them have arisen a variety of software packages, each typically tailored to training specific architectures. Given their unique specialisations, these individual implementations do not normally conform to a common interface, making it difficult for practitioners to migrate training and validation pipelines between different architectures.

![Schematic overview of the functionality of `graph-pes`. The core components are highlighted in colour. Red: The `AtomicGraph` class is used to represent atomic structures and incorporates the notion of locality via a neighbour list. Blue: The `GraphPESModel` class is the general base class for all `graph-pes` models. Green: `graph-pes` includes a CLI for easy training, and interfaces to multiple external simulation tools for evaluating MLIPs.](./overview.png)

`graph-pes` provides a **unified interface and framework** for defining, training, and working with graph-based MLIP models. This reduces the barrier to entry for researchers wanting to implement new MLIP architectures, and allows practitioners to easily explore different MLIP architectures: training scripts require as little as one line of code to swap between model architectures, while validation scripts can be written in an architecture-agnostic manner, with `LAMMPS` input scripts, `ASE` calculators, and `torch-sim` simulations requiring no changes other than pointing to a different model file.

# Related work

`graph-pes` is beginning to drive projects within our research group, and we hope that it will be useful to many others. In recent work, we have described the use of `graph-pes` for fitting NequIP models to datasets created using `autoplex` [@Liu-25-08], for assessing zero-shot performance of different graph-network MLIPs [@Mahmoud-25-02], and for distilling atomistic foundation models [@Gardner-25-06].

Relevant alternative packages that offer training and validation for _specific_ ML-PES architectures include: `schnetpack` [@schutt2019schnetpack; @schutt2023schnetpack], `deepmd-kit` [@Wang-18-07; @Zeng-23-08], `nequip` [@Batzner-22-05], `mace-torch` [@Batatia-22-10], `torchmd-net` [@TorchMDNet], and `fairchem` [@fairchem]. The `MatterTune` package [@Kong-25-04] provides a unified interface for fine-tuning atomistic foundation models, but does not create models with a common interface, or allow for training arbitrary MLIP architectures from scratch.

# Features and implementation

## Representing atomic structures with graphs

Graphs are a natural way to represent atomic structure, with nodes representing atoms and edges defining a local neighbourhood for each atom.
The `AtomicGraph` class therefore serves as the base data structure in `graph-pes`, storing atomic positions, chemical identities, unit cell vectors (where applicable), and an edge list.
Writing performant code for graph operations can be challenging; 
`graph-pes` therefore provides optimised implementations for accessing derived properties and performing common operations on both single and batched graph instances. 
This simplifies the implementation of new MLIP models and ensures forward passes remain readable. 
Full API details are available in the project documentation.

## Model implementations

All MLIP models in `graph-pes` are implemented as subclasses of the `GraphPESModel` base class. 
Implementations need only define a forward pass that returns a local energy for each atom or a total energy for the structure; 
the framework handles the calculation of forces and stress tensors in a conservative manner via automatic differentiation. 
We also support models that return direct force and stress tensor predictions (e.g., `TensorNet` or `orb-v3-*` with their optional direct force readout heads).

Building on the `GraphPESModel` class, we provide independent (re-) implementations of popular MLIP architectures, including `PaiNN` [@Schutt-21-06], `EDDP` [@Pickard-22-07], `NequIP` [@Batzner-22-05], `MACE` [@Batatia-22-10], and `TensorNet` [@Simeon-23-06]. 
We use building blocks provided by `e3nn` [@Geiger-22-07] to implement models that act on spherical tensor decompositions.

Furthermore, we provide an `AdditionModel` implementation, which makes predictions as a sum over independent models. 
This allows `graph-pes` to add offset energies (`EnergyOffset`) and pair-repulsion terms (`LennardJones`, `Morse`, and `ZBLCoreRepulsion`) to any model implementation, as well as the creation and use of model ensembles.

## Training and validation

We provide the `graph-pes-train` CLI tool for training any `GraphPESModel` on datasets of labelled atomic structures.
As well as training from scratch, we also support the fine-tuning of existing models on new datasets, facilitating a variety of strategies, including synthetic pre-training [@Gardner-24-01], foundation model fine-tuning (see below), and frozen transfer learning [@Radova-25-02].

Under the hood, `graph-pes-train` builds upon the `PyTorch Lightning` [@Lightning] training loop, allowing the user to configure a variety of common training features and callbacks.
We also support the use of arbitrary, user-defined components, including custom loss functions, model architectures, optimisers, and datasets.

Because all models conform to the same interface, all training features can be used with any model architecture. Similarly, all downstream model uses can be written in an architecture-agnostic manner, allowing for MD, relaxations, and other scripts to be written once, and then used with any MLIP architecture, _e.g._ for extended validation beyond simple error metrics [@Morrow-23-03].

## Easy access to foundation models

A recent area of research is the development of "foundational" MLIPs that can describe the potential-energy surface of a wide range of systems. `graph-pes` integrates directly with the `mace-torch`, `mattersim`, and `orb-models` packages to provide access to, among others, the `MACE-MP` [@Batatia-25-11], `MatterSim` [@Yang-24-05], `orb-v2` [@Neumann-24-10], `MACE-OFF` [@Kovacs-25-01],  `Egret-v1` [@Mann-25-05], and `orb-v3` [@Rhodes-25-04] families of models. Each of these integrations generates `GraphPESModels` that are directly compatible with all relevant `graph-pes` features.

# Acknowledgements

We thank Zo√© Faure Beaulieu, Krystian Gierczak, and Daniel Thomas du Toit for early testing and feedback.
J.L.A.G. acknowledges support from an EPSRC DTP award [grant number EP/T517811/1], and from the Department of Chemistry, University of Oxford.
This work was supported by UK Research and Innovation [grant number EP/X016188/1].

# References
